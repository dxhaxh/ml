{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.core.debugger import set_trace #this is used for debugging\n",
    "np.random.seed(1234) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Gradient\n",
    "We implement the gradient function in the code below. We want the entire gradient, in the form of a matrix, the same shape as $W$. Note that although gradient is generally in the form of a vector, here, for convenience we are keeping its shape the same as the weight matrix, rather than converting it to a vector. The mathematical expression for the gradient, when using the design matrix $X$, and the one-hot coding of labels $y$ is:\n",
    "\n",
    "$$\n",
    " g = \\frac{1}{N} X^\\top (\\mathcal{S}(X w) - y) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ''' \n",
    "    calculate the softmax function along the last dimension\n",
    "    '''\n",
    "    z0 = np.max(z,-1, keepdims=True) \n",
    "    z_exp = np.exp(z - z0) #to prevent underflow/overflow\n",
    "    p = z_exp / np.sum(z_exp, -1, keepdims=True)\n",
    "    return p\n",
    "\n",
    "def cost(\n",
    "    x, # N x D \n",
    "    yhot, # N x C: one hot coding of the labels\n",
    "    w # D x C\n",
    "        ):\n",
    "    N, D = x.shape                    \n",
    "    p = np.softmax(x @ w)\n",
    "    ce_loss = -np.mean(np.sum(yhot * np.log(p), -1)) # the cost function L discussed above\n",
    "    return ce_loss\n",
    "\n",
    "def gradient(\n",
    "    x, # NxD\n",
    "    yhot, # NxC\n",
    "    w # DxC\n",
    "        ):\n",
    "    \n",
    "    N, D = x.shape\n",
    "    grad = (1/N) * x.transpose() @ (softmax(x @ w) - yhot)\n",
    "    \n",
    "    return grad  # D x C matrix: the same shape as the weight matrix w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification using Softmax Regression\n",
    "We want to implement softmax regression, including the stochastic gradient descent using momentum to fit the training data. Our training data is in the form of $8x8$ image of digits 0,1,2,...,9. Therefore each input $x$ has 64 features corresponding to pixel values. The input to our classifier is therefore a vector of length 64, and the output is class probabilities. Let's load this dataset and split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x20000 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid                         #To plot the images in grid \n",
    "\n",
    "x, y = load_digits(return_X_y=True)\n",
    "split = (80,20) # percentage for train, and test split\n",
    "n = x.shape[0]\n",
    "n_tr, n_ts = n*split[0]//100, n*split[1]//100\n",
    "x_tr, x_ts = x[:n_tr], x[n_tr:n_tr+n_ts] \n",
    "y_tr, y_ts = y[:n_tr], y[n_tr:n_tr+n_ts] \n",
    "\n",
    "def plot_digits(x,y):\n",
    "    num_plots = x.shape[0]\n",
    "    fig = plt.figure(figsize=(num_plots, 10.*num_plots))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1, num_plots), axes_pad=0.1)\n",
    "    for i in range(num_plots):\n",
    "        grid[i].imshow(x[i].reshape((8,8)))\n",
    "        grid[i].set_title(str(y[i]))\n",
    "    plt.show()\n",
    "plot_digits(x[:20], y[:20])  #plot 20 first instances in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum\n",
    "Below we complete the implementation of the gradient descent using momentum. We have a class that records hyper-parameters of the optimization including learning rate and momentum parameter, as well as the gradient function `grad_fn`. Each call to the step function should update the input weight `w` by calculating the gradient and using the exponential running average of the past gradients that is stored in `m`. The updated weight `wp` is returned by the `step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOpt:\n",
    "    '''\n",
    "    Implements the method of momentum for linear regression and classification.\n",
    "    Each step takes the gradient function and its arguments, including w_{t} as input, \n",
    "    updates the exponential running average of gradients and returns w_{t+1}.\n",
    "    '''\n",
    "    def __init__(self, grad_fn, learning_rate=.001, beta = .95):\n",
    "        self.grad_fn = grad_fn # the gradient function: grad_fn(x,y, w)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.m = None #the exponential running average of gradients\n",
    "            \n",
    "    def step(\n",
    "        self, \n",
    "        x, # N x D \n",
    "        yhot, # N x C\n",
    "        w # D+1 is the w at current time step. The first element w_0 is the bias and the rest are the weights \n",
    "        ):\n",
    "        \n",
    "        if type(self.m)==type(None):\n",
    "            self.m = (1-self.beta)*self.grad_fn(x, yhot, w)\n",
    "        else:\n",
    "            self.m = self.beta*self.m + (1-self.beta)*self.grad_fn(x, yhot, w)\n",
    "                 \n",
    "        wp = w-self.learning_rate*self.m\n",
    "        \n",
    "        return wp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Mini-Batch of Data\n",
    "Now, we are ready to implement the `SoftmaxRegression` class. We have two methods, `fit`, which fits the training data by finding the model parameter `w` that minimizes the empirical risk, and the `predict` method which produces class probabilities for a given input.\n",
    "\n",
    "Here, for the training we want to use mini-batch of size `minibatch`. Each gradient step that is performed using the optimizer `step` function that we previously implemented, should therefore use a minibatch rather than the entire trainin data for its calculations. When training using mini-batch we use the term `epoch` rather than *iteration* to refer to several iterations using mini-batch that covers the entire training data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x, C=None): \n",
    "    if C is None:\n",
    "        C = np.max(x) + 1\n",
    "    N = x.shape[0]\n",
    "    x_hot = np.zeros((N,C))\n",
    "    x_hot[np.arange(N), x] = 1 #use the value of x to index the last dimension\n",
    "    return x_hot \n",
    "\n",
    "class SoftmaxRegression:\n",
    "\n",
    "    def __init__(self, max_epochs = 1000, eps = 1e-3, minibatch=64):\n",
    "        self.max_epochs = max_epochs \n",
    "        self.eps = eps\n",
    "        self.b = minibatch\n",
    "        self.w = None # weights\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        yhot = onehot(y)\n",
    "        D, C = x_tr.shape[1], yhot.shape[1] \n",
    "        self.w = np.zeros((D,C))        \n",
    "        for epoch in range(self.max_epochs): \n",
    "            w_old = self.w\n",
    "            for i in range(n_tr//self.b): # update the weights using i'th mini-batch\n",
    "                \n",
    "                self.w = optimizer.step(x[i*self.b:(i+1)*self.b], yhot[i*self.b:(i+1)*self.b], self.w)\n",
    "                \n",
    "            if np.linalg.norm(self.w - w_old) < self.eps: # check for convergence\n",
    "                print(f'converged after {epoch} epochs!')\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        p = softmax(x @ self.w)  #predict probabilities \n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print the training and test accuracy of the model using default hyper-parameters. Plot the confusion matrix on the test set (using either `scikit-learn` implementation or the one provided in the [Model Selection Tutorial](https://github.com/mravanba/comp451/blob/main/ModelSelection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged after 516 epochs!\n",
      "training accuracy: 0.9972164231036882 \n",
      "test accuracy 0.9136490250696379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a, b = x_tr.shape\n",
    "x0 = np.ones((a, 1))\n",
    "x_tr = np.hstack((x0, x_tr))                     #adding column of 1's to x so sides match accounting for bias\n",
    "\n",
    "a, b = x_ts.shape\n",
    "x0 = np.ones((a, 1))\n",
    "x_ts = np.hstack((x0, x_ts))                     #adding column of 1's to x so sides match accounting for bias\n",
    "\n",
    "\n",
    "sfm=SoftmaxRegression()\n",
    "opt=MomentumOpt(gradient)\n",
    "\n",
    "sfm.fit(x_tr, y_tr, opt)\n",
    "\n",
    "probs_tr = sfm.predict(x_tr) #pred_tr is a vector of probabilities, we want to output the max probability\n",
    "pred_tr = np.argmax(probs_tr, -1)\n",
    "acc_tr = np.mean(pred_tr == y_tr)\n",
    "\n",
    "probs_ts = sfm.predict(x_ts)\n",
    "pred_ts = np.argmax(probs_ts, -1)\n",
    "acc_ts = np.mean(pred_ts == y_ts)\n",
    "\n",
    "print(f'training accuracy: {acc_tr} \\ntest accuracy {acc_ts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33.  0.  0.  0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. 30.  0.  1.  0.  0.  0.  0.  0.  5.]\n",
      " [ 0.  0. 35.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. 28.  0.  2.  0.  1.  5.  0.]\n",
      " [ 0.  0.  0.  0. 34.  0.  0.  0.  0.  3.]\n",
      " [ 0.  0.  0.  0.  0. 37.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0. 36.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0. 34.  1.  0.]\n",
      " [ 0.  2.  0.  0.  1.  1.  0.  0. 27.  1.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  0.  0. 34.]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(y, yh):\n",
    "    n_classes = np.max(y) + 1\n",
    "    c_matrix = np.zeros((n_classes, n_classes))\n",
    "    for c1 in range(n_classes):\n",
    "        for c2 in range(n_classes):\n",
    "            #(y==c1)*(yh==c2) is 1 when both conditions are true or 0\n",
    "            c_matrix[c1, c2] = np.sum((y==c1)*(yh==c2))\n",
    "    return c_matrix\n",
    "\n",
    "cmat = confusion_matrix(y_ts, pred_ts)\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
